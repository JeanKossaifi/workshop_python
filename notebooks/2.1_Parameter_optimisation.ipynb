{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Exercise: parameter optimisation\u00b6"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This exercise covers cross-validation of regression models on the Diabetes dataset. The diabetes data consists of 10 physiological variables (age, sex, weight, blood pressure) measure on 442 patients, and an indication of disease progression after one year:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from sklearn.datasets import load_diabetes\n",
      "data = load_diabetes()\n",
      "X, y = data.data, data.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(X.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(y.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we'll be fitting two regularized linear models, Ridge Regression, which uses \u21132 regularlization, and Lasso Regression, which uses \u21131 regularization."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from sklearn.linear_model import Ridge, Lasso"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll first use the default hyper-parameters to see the baseline estimator. We'll use the cross-validation score to determine goodness-of-fit."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score\n",
      "\n",
      "model = Ridge()\n",
      "print('Score with Ridge Regression: {}.'.format(cross_val_score(model, X, y).mean()))\n",
      "\n",
      "model = Lasso()\n",
      "print('Score with Lasso Regression: {}.'.format(cross_val_score(model, X, y).mean()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that for the default hyper-parameter values, Lasso outperforms Ridge. But is this the case for the optimal hyperparameters of each model?\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Exercise: Basic Hyperparameter Optimization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here spend some time writing a function which computes the cross-validation score as a function of alpha, the strength of the regularization for Lasso and Ridge. We'll choose 20 values of alpha between 0.0001 and 1:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "alphas = np.logspace(-3, -1, 30)\n",
      "\n",
      "# plot the mean cross-validation score for a Ridge estimator and a Lasso estimator\n",
      "# as a function of alpha.  Which is more difficult to tune?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Solution "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# %load solutions/07B_basic_grid_search.py\n",
      "for Model in [Lasso, Ridge]:\n",
      "    scores = [cross_val_score(Model(alpha), X, y, cv=3).mean()\n",
      "              for alpha in alphas]\n",
      "    best_index = np.argmax(scores)\n",
      "    best_alpha = alphas[best_index]\n",
      "    plt.axvline(best_alpha, color='k', linestyle='--')\n",
      "    plt.plot(alphas, scores, label='{0}, best alpha={1:.3f}'.format(Model.__name__, best_alpha))\n",
      "    \n",
      "plt.legend(loc='lower left')\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Automatically Performing Grid Search\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Because searching a grid of hyperparameters is such a common task, scikit-learn provides several hyper-parameter estimators to automate this. We'll explore this more in depth later in the tutorial, but for now it is interesting to see how GridSearchCV works:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from sklearn.grid_search import GridSearchCV"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "GridSearchCV is constructed with an estimator, as well as a dictionary of parameter values to be searched. We can find the optimal parameters this way:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for Model in [Ridge, Lasso]:\n",
      "    gscv = GridSearchCV(Model(), dict(alpha=alphas), cv=3).fit(X, y)\n",
      "    print(Model.__name__, gscv.best_params_)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Built-in Hyperparameter Search"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For some models within scikit-learn, cross-validation can be performed more efficiently on large datasets. In this case, a cross-validated version of the particular model is included. The cross-validated versions of Ridge and Lasso are RidgeCV and LassoCV, respectively. The grid search on these estimators can be performed as follows:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import RidgeCV, LassoCV\n",
      "for Model in [RidgeCV, LassoCV]:\n",
      "    model = Model(alphas=alphas, cv=3).fit(X, y)\n",
      "    print(Model.__name__, model.alpha_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that the results match those returned by GridSearchCV."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}